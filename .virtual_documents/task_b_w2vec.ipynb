import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


import time


import spacy


spacy.prefer_gpu()


import torch
import torch.nn as nn
import torch.nn.functional as F


cuda_av = torch.cuda.is_available()
cuda_av


if cuda_av:
    cuda_id = torch.cuda.current_device()
    print(f'ID urządzenia CUDA: {cuda_id}')
    print(f"Nazwa urządzenia CUDA: {torch.cuda.get_device_name(cuda_id)}")


# Tworzenie kodu uniwersalnego: dla CUDA i CPU
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device


tweets = pd.read_csv('data/olid-training-v1.0.tsv', sep='\t')


tweets.head()


len(tweets)


tweets = tweets.dropna(subset=['subtask_b'])


len(tweets)


nlp = spacy.load('en_core_web_sm')


# dodawanie kolumny z tokenami
tweets['tokens'] = tweets['tweet'].apply(nlp)


tweets.iloc[:3]


# dodawanie kolumny z lematami za pomoca funkcji lambda - przy tym usuwanie:
# odwolan do wczesniejszych tweetow (zawiera @)
# - slowa url (czyli adresy stron inernetowych które w danych wejściowych mają URL zamiast adreasu.
# # - hasztagów; przyimków i innych częstych słów (stop words) oraz znaków interpunkcyjnych.
# spacji ' ', '  ', '   '

# emotikony zostawiam - uważam, że też niosą znaczenie

tweets['tokens'] = tweets['tweet'].apply(nlp)
tweets['lemmas'] = tweets['tokens'].apply \
    (lambda list_tokens : [token.lemma_.strip()
                           for token in list_tokens if ('@' not in token.lemma_
                           and '#' not in token.lemma_ and 'url' not in token.lemma_
                           and not token.is_stop and not token.is_punct and token.lemma_ != ' '
                           and token.lemma_ != '  ' and token.lemma_ != '   '
                           and token.lemma_ != '    ' and token.lemma_.strip() != '')])


tweets.iloc[:3]


tweets['length_lemmas'] = tweets['lemmas'].apply(lambda row: len(row))


tweets.loc[:10, 'length_lemmas']


max_length_tweet = max(tweets['length_lemmas'])


test_level_b = pd.read_csv('data/testset-levelb.tsv', sep='\t')


labels_level_b = pd.read_csv('data/labels-levelb.csv', header=None)


labels_level_b = labels_level_b.rename(columns={0:'id', 1:'subtask_b'})


test_b = pd.merge(test_level_b, labels_level_b, on = "id", how = "inner")


test_b.head()


len(test_b)


test_b = test_b.dropna(subset=['subtask_b'])


len(test_b)


test_b['tokens'] = test_b['tweet'].apply(nlp)
test_b['lemmas'] = test_b['tokens'].apply \
    (lambda list_tokens : [token.lemma_.strip() \
                           for token in list_tokens if ('@' not in token.lemma_
                           and '#' not in token.lemma_ and 'url' not in token.lemma_
                           and not token.is_stop and not token.is_punct and token.lemma_ != ' '
                           and token.lemma_ != '  ' and token.lemma_ != '   '
                           and token.lemma_ != '    ' and token.lemma_.strip() != '')])


test_b['length_lemmas'] = test_b['lemmas'].apply(lambda row: len(row))


max(test_b['length_lemmas'])


test_b.head()


len(test_b)


max_length_tweet_test = max(test_b['length_lemmas'])


max_length_tweet_test


max_width = max(max_length_tweet, max_length_tweet_test)


def convert_words_to_numbers(lemmas_series, dict_ = dict()):
    # dict_ = dict()
    for row in lemmas_series:
        for lemma in row:
            if lemma not in dict_:
                dict_[lemma] = len(dict_) + 1

    return dict_


def lemmas_to_numbers(row, max_list_lemmas, dict_):
    list_numbers = []
    for i in range(max_list_lemmas - len(row)):
        list_numbers.append(0)

    for lemma in row:
        list_numbers.append(dict_[lemma])

    array_numbers = np.array(list_numbers, dtype=np.int32)
    return array_numbers


dict_lemmas = convert_words_to_numbers(tweets['lemmas'])


sorted(dict_lemmas)[:3]


# zamiana lematu na liczbe ze slownika ktory odpowiada danemu slowu
tweets['numbers'] = tweets['lemmas'].apply(lambda row : lemmas_to_numbers(row, max_width, dict_lemmas))


tweets.loc[0:6, 'numbers']


# przypisywanie X_train kolumny numbers skonwertowanej na tablice numpy
X_train = tweets['numbers'].values


# łączenie wierszy tablic w jedną tablicę 2D
X_train = np.stack(X_train)


# konwertowanie tablicy 2d do tensora
X_train = torch.FloatTensor(X_train)


# ustawianie kolumn z etykietami na poszczególne zadania jako type które przechowują kategorie


cat_cols = ['subtask_a', 'subtask_b', 'subtask_c']


for col in cat_cols:
    tweets[col] = tweets[col].astype('category')


tweets['labels_b'] = tweets['subtask_b'].cat.codes


# OFFENSIVE jest jako 1, a NOT OFFENSIVE jest jako 0
tweets.loc[:5, 'labels_b']


tweets


tweets.iloc[0]


y_train = tweets['labels_b'].values


y_train = torch.tensor(y_train).reshape(-1,1)


y_train[:3]


data = torch.cat((X_train, y_train), axis=1)


data = data.to(device)


dict_lemmas = convert_words_to_numbers(test_b['lemmas'], dict_lemmas)


sorted(dict_lemmas)[:5]


# zamiana lematu na liczbe ze slownika ktory odpowiada danemu slowu
test_b['numbers'] = test_b['lemmas'].apply(lambda row : lemmas_to_numbers(row, max_width, dict_lemmas))


test_b['lemmas'].head()


test_b.loc[:5, 'numbers']





X_test = test_b['numbers'].values


X_test = np.stack(X_test)


X_test = torch.FloatTensor(X_test)


# cat_cols = ['subtask_a', 'subtask_b', 'subtask_c']
# for col in cat_cols:
#     X_test[col] = X_test[col].astype('category')


test_b['subtask_b'] = test_b['subtask_b'].astype('category')


test_b['labels_b'] = test_b['subtask_b'].cat.codes


# OFFENSIVE jest jako 1, a NOT OFFENSIVE jest jako 0
test_b.loc[:5, 'labels_b']


y_test = test_b['labels_b'].values


y_test = torch.tensor(y_test).reshape(-1,1)


y_test[:5]


data_test_b = torch.cat((X_test, y_test), axis=1)


data_test_b = data_test_b.to(device)


tweets.loc[[0,1], ['subtask_b', 'labels_b']]


test_b.loc[[0,4], ['subtask_b', 'labels_b']]


embeddings = []


embeddings.append(np.zeros(96))


for key, value in dict_lemmas.items():
    embeddings.append(nlp(key)[0].vector)
    # print(key)


# embeddings


len(dict_lemmas)


dict_lemmas_inverted = {v: k for k, v in dict_lemmas.items()}


# dict_lemmas_inverted


# sprawdzanie embeddingów - czy się dobrze zapisały
for key, value in dict_lemmas.items():
    comparison = embeddings[value] == nlp(key)[0].vector
    if comparison.all() == False:
        print('false')

print('finished')



embeddings[1][:10]


emb_torch = torch.tensor(embeddings, dtype=torch.float32, device=device)


emb_torch.shape


class Model(nn.Module):
    def __init__(self, emb_vectors, in_features=18, h1=80, h2=50, h3=None, embedding_dim=None, out_features=1):
        super().__init__()

        # warstwa embeddingów wczytująca embeddingi przekazane przy tworzeniu modelu
        self.embedding = nn.Embedding.from_pretrained(emb_vectors)

        # self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # dropout layer - losowe pomijanie uczenia się pewnych neuronów
        self.dropout = nn.Dropout(0.3)

        self.fc1 = nn.Linear(embedding_dim * in_features, h1)
        self.fc2 = nn.Linear(h1, h2)
        if h3 is None:
            self.out = nn.Linear(h2, out_features)
        # self.sig = nn.Sigmoid()
        else:
            self.fc3 = nn.Linear(h2, h3)
            self.out = nn.Linear(h3, out_features)


    def forward(self, x):
        # print(x.shape)
        x = x.long()
        # print(x.shape)
        embeds = self.embedding(x)
        
        # print(embeds.shape)
        embeds = embeds.view(embeds.shape[0], -1)
        # print(embeds.shape)
        x = torch.sigmoid(self.fc1(embeds))
        
        x = self.dropout(x)
        
        # x = self.dropout(x)

        x = torch.sigmoid(self.fc2(x))
        x = self.dropout(x)

        if h3 is not None:
            x = torch.sigmoid(self.fc3(x))
            x = self.dropout(x)
        x = torch.sigmoid(self.out(x))

        return x


print('max width: 73')


epochs = 100
learning_rate = 0.005
batch_size = 500
vocab_size = len(dict_lemmas) + 1
embedding_dim = emb_torch.shape[1]
h1 = 2
h2 = 3
h3 = None


train_dataset_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)
test_dataset_loader_b = torch.utils.data.DataLoader(data_test_b, batch_size=batch_size, shuffle=True)


# tworzenie numpy array z listy liczb (odpowiadających lematom)


max_width


torch.manual_seed(32)
model = Model(emb_torch, max_width, h1, h2, h3, embedding_dim)


model = model.to(device)


criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)


model


losses = []
losses_test = []
accuracy = []
accuracy_test = []
train_count = len(tweets)
test_count_a = len(test_b)


save_every_epoch = int(epochs/10)
losses_imp = []
losses_test_imp = []
accuracy_imp = []
accuracy_test_imp = []


start_time = time.time()
for e in range(epochs):
    loss_epoch = torch.empty(0)
    correct_epoch = 0

    loss_epoch_test = np.empty(0)
    correct_epoch_test = 0

    for batch_num, batch in enumerate(train_dataset_loader):
        X_train = batch[:,:-1]
        y_train = batch[:,-1].reshape(-1,1)
        y_pred = model.forward(X_train)

        loss = criterion(y_pred, y_train)
        if (e == 0 and batch_num == 0):
            print(f'Loss on the first batch: {loss}')

        # print(loss.item())
        loss_epoch = np.append(loss_epoch, loss.detach().cpu().numpy())
        # losses.append(loss.item())


        optimizer.zero_grad()
        loss.backward()
        optimizer.step()


        predicted = torch.round(y_pred)
        # print(y_train)
        predicted = (predicted == y_train).sum().cpu()
        pred_cpu = predicted.cpu()
        # print(f'pred_cpu: {pred_cpu}')
        correct_epoch += predicted

# przechodzenie przez dane testowe
    with torch.no_grad():
        for batch_num_test, batch_test in enumerate(test_dataset_loader_b):
            X_test = batch_test[:,:-1]
            y_test = batch_test[:,-1].reshape(-1,1)
            y_pred_test = model.forward(X_test)

            loss_test = criterion(y_pred_test, y_test)
            if (e == 0 and batch_num_test == 0):
                print(f'Loss on the first batch on test data: {loss_test}')
                print(loss_test.shape)

            # print(loss.item())
            loss_epoch_test = np.append(loss_epoch_test, loss_test.item())
            # print(loss_test)
            # losses.append(loss.item())

            predicted = torch.round(y_pred_test)
            # print(predicted)
            # print(y_test)
            predicted = (predicted == y_test).sum()
            pred_cpu = predicted.cpu()
            # print(f'pred_cpu: {pred_cpu}')
            correct_epoch_test += predicted

        loss_epoch_test = loss_epoch_test.sum() / test_count_a
        losses_test.append(loss_epoch_test)
        accuracy_epoch_test = correct_epoch_test * 100/test_count_a
        accuracy_epoch_test_cpu = accuracy_epoch_test.cpu()
        accuracy_test.append(accuracy_epoch_test_cpu)

        if e % save_every_epoch == 0:
            losses_test_imp.append((e, loss_epoch_test))
            accuracy_test_imp.append((e, accuracy_epoch_test_cpu))

    loss_epoch = loss_epoch.sum() / train_count
    losses.append(loss_epoch)
    accuracy_epoch = correct_epoch * 100/train_count
    accuracy_epoch_cpu = accuracy_epoch.cpu()
    accuracy.append(accuracy_epoch)

    if e % save_every_epoch == 0:
        losses_imp.append((e, loss_epoch))
        accuracy_imp.append((e, accuracy_epoch_cpu))


    if e % 5 == 0:
        print(f' epoch: {e} | loss: {loss_epoch} | predicted: {correct_epoch} | accuracy: {accuracy_epoch} | test loss: {loss_epoch_test} | '
              + f'predicted: {correct_epoch_test} | test accuracy: {accuracy_epoch_test}')

duration = time.time() - start_time


## Plot loss functions over epochs
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,7))

ax1.plot(range(epochs), losses, label="Training data")
ax1.plot(range(epochs), losses_test, label="Test data")
ax1.set_title('Losses and epochs')

ax1.legend(loc="upper right")

ax2.plot(range(epochs), accuracy, label="Training data")
ax2.plot(range(epochs), accuracy_test, label="Test data")
ax2.set_title("Accuracy and epochs")

ax2.legend(loc="lower right")

ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')

ax2.set_xlabel('Epoch')
ax2.set_ylabel('Accuracy')

plt.show()


# dictionary storing the data
summary = {
    "Best": [min(losses), max(accuracy).item(), min(losses_test), max(accuracy_test).item()],
}


for i in range(len(losses_imp)):
    tr = 'Training ', losses_imp[i][0]
    tt = 'Testing ', losses_test_imp[i][0]
    summary[losses_imp[i][0]] = [losses_imp[i][1], accuracy_imp[i][1].item(), losses_test_imp[i][1], accuracy_test_imp[i][1].item()]


# dataframe from dict
summary_df = pd.DataFrame.from_dict(summary, orient='index', columns=['Loss', 'Accuracy', 'Loss test', 'Accuracy test'])


print('Spacy embeddings')
print(model)
print('learning_rate: ', learning_rate)
print('batch_size: ', batch_size)
print('time: ', duration)


summary_df
